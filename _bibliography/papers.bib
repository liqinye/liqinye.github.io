---
---

@inproceedings{ye2025calibratingpretrainedlanguageclassifiers,
      title={Calibrating Pre-trained Language Classifiers on LLM-generated Noisy Labels via Iterative Refinement}, 
      author={Liqin Ye and Agam Shah and Chao Zhang and Sudheer Chava},
      year={2025},
      arxiv={2505.19675},
      url={https://arxiv.org/abs/2505.19675}, 
      booktitle={ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
      abbr={KDD},
      code={https://github.com/gtfintechlab/SiDyP},
      selected={true},
      abstract={The traditional process of creating labeled datasets is labor-intensive and expensive. Recent breakthroughs in open-source large language models (LLMs) have opened up a new avenue in generating labeled datasets automatically for various natural language processing (NLP) tasks, providing an alternative to such an expensive annotation process. However, the reliability of such auto-generated labels remains a significant concern due to inherent inaccuracies. When learning from noisy labels, the model's generalization is likely to be harmed as it is prone to overfit to those label noises. While previous studies in learning from noisy labels mainly focus on synthetic noise and real-world noise, LLM-generated label noise receives less attention. In this paper, we propose SiDyP: Simplex Label Diffusion with Dynamic Prior to calibrate the classifier's prediction, thus enhancing its robustness towards LLM-generated noisy labels. SiDyP retrieves potential true label candidates by neighborhood label distribution in text embedding space and iteratively refines noisy candidates using a simplex diffusion model. Our framework can increase the performance of the BERT classifier fine-tuned on both zero-shot and few-shot LLM-generated noisy label datasets by an average of 7.21\% and 7.30\% respectively. We demonstrate the effectiveness of SiDyP by conducting extensive benchmarking for different LLMs over a variety of NLP tasks.}
}

@inproceedings{ye2025preciseattributeintensitycontrol,
      title={Precise Attribute Intensity Control in Large Language Models via Targeted Representation Editing}, 
      author={Rongzhi Zhang* and Liqin Ye* and Yuzhao Heng and Xiang Chen and Tong Yu and Lingkai Kong and Sudheer Chava and Chao Zhang},
      year={2025},
      url={https://arxiv.org/abs/2505.19675}, 
      booktitle={An arXiv version will be available soon},
      abbr={NeurIPS},
      selected={true},
      under_review={true}
}

@inproceedings{shah2025reportedcutofflargelanguage,
      title={Beyond the Reported Cutoff: Where Large Language Models Fall Short on Financial Knowledge}, 
      author={Agam Shah and Liqin Ye and Sebastian Jaskowski and Wei Xu and Sudheer Chava},
      year={2025},
      arxiv={2504.00042},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.00042}, 
      booktitle={Under Review},
      abbr={COLM},
      selected={true},
      under_review={true},
      abstract={Large Language Models (LLMs) are frequently utilized as sources of knowledge for question-answering. While it is known that LLMs may lack access to real-time data or newer data produced after the model's cutoff date, it is less clear how their knowledge spans across historical information. In this study, we assess the breadth of LLMs' knowledge using financial data of U.S. publicly traded companies by evaluating more than 197k questions and comparing model responses to factual data. We further explore the impact of company characteristics, such as size, retail investment, institutional attention, and readability of financial filings, on the accuracy of knowledge represented in LLMs. Our results reveal that LLMs are less informed about past financial performance, but they display a stronger awareness of larger companies and more recent information. Interestingly, at the same time, our analysis also reveals that LLMs are more likely to hallucinate for larger companies, especially for data from more recent years. We will make the code, prompts, and model outputs public upon the publication of the work.}
}

@inproceedings{shah2025wordsuniteworldunified,
      title={Words That Unite The World: A Unified Framework for Deciphering Central Bank Communications Globally}, 
      author={Agam Shah and Siddhant Sukhani and Huzaifa Pardawala and Saketh Budideti and Riya Bhadani and Rudra Gopal and Siddhartha Somani and Michael Galarnyk and Soungmin Lee and Arnav Hiray and Akshar Ravichandran and Eric Kim and Pranav Aluru and Joshua Zhang and Sebastian Jaskowski and Veer Guda and Meghaj Tarte and Liqin Ye and Spencer Gosden and Rutwik Routu and Rachel Yuh and Sloka Chava and Sahasra Chava and Dylan Patrick Kelly and Aiden Chiang and Harsit Mittal and Sudheer Chava},
      year={2025},
      arxiv={2505.17048},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.17048},
      booktitle={Under Review},
      abbr={NeurIPS D\&B Track},
      code={https://github.com/gtfintechlab/WorldCentralBanks},
      huggingface={https://huggingface.co/gtfintechlab},
      website={https://gcb-web-bb21b.web.app/},
      selected={true},
      under_review={true},
      abstract={Central banks around the world play a crucial role in maintaining economic stability. Deciphering policy implications in their communications is essential, especially as misinterpretations can disproportionately impact vulnerable populations. To address this, we introduce the World Central Banks (WCB) dataset, the most comprehensive monetary policy corpus to date, comprising over 380k sentences from 25 central banks across diverse geographic regions, spanning 28 years of historical data. After uniformly sampling 1k sentences per bank (25k total) across all available years, we annotate and review each sentence using dual annotators, disagreement resolutions, and secondary expert reviews. We define three tasks: Stance Detection, Temporal Classification, and Uncertainty Estimation, with each sentence annotated for all three. We benchmark seven Pretrained Language Models (PLMs) and nine Large Language Models (LLMs) (Zero-Shot, Few-Shot, and with annotation guide) on these tasks, running 15,075 benchmarking experiments. We find that a model trained on aggregated data across banks significantly surpasses a model trained on an individual bank's data, confirming the principle "the whole is greater than the sum of its parts." Additionally, rigorous human evaluations, error analyses, and predictive tasks validate our framework's economic utility.}
}